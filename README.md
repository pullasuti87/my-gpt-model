# my_gpt (simple transformerLM)

to understand language models a little better,
relies heavily on the _Building Transformer Models with Attention_ book
a_nd _Andrej Karpathy's_ transformer implementations

- self-attention
- multi-head attention
- position embeddings
- tiktoken
- uses data.txt as training -> suomi text

## todo 
- no optimizer 
- tests 

[check out](notes.md)


